import tensorflow as tf 
from tensorflow import keras 
from keras import layers 

import numpy as np 
import matplotlib.pyplot as plt 

import warnings 
warnings.filterwarnings('ignore')



# Load in the data 
cifar100 = tf.keras.datasets.cifar100 

# Distribute it to train and test set 
(x_train, y_train), (x_val, y_val) = cifar100.load_data() 
print(x_train.shape, y_train.shape, x_val.shape, y_val.shape) 



def show_samples(data, labels): 
	plt.subplots(figsize=(10, 10)) 
	for i in range(12): 
		plt.subplot(3, 4, i+1) 
		k = np.random.randint(0, data.shape[0]) 
		plt.title(labels[k]) 
		plt.imshow(data[k]) 
	plt.tight_layout() 
	plt.show() 


show_samples(x_train, y_train) 



y_train = tf.one_hot(y_train, 
					depth=y_train.max() + 1, 
					dtype=tf.float64) 
y_val = tf.one_hot(y_val, 
				depth=y_val.max() + 1, 
				dtype=tf.float64) 

y_train = tf.squeeze(y_train) 
y_val = tf.squeeze(y_val) 



model = tf.keras.models.Sequential([ 
	layers.Conv2D(16, (3, 3), activation='relu', 
				input_shape=(32, 32, 3), padding='same'), 
	layers.Conv2D(32, (3, 3), 
				activation='relu', 
				padding='same'), 
	layers.Conv2D(64, (3, 3), 
				activation='relu', 
				padding='same'), 
	layers.MaxPooling2D(2, 2), 
	layers.Conv2D(128, (3, 3), 
				activation='relu', 
				padding='same'), 


	layers.Flatten(), 
	layers.Dense(256, activation='relu'), 
	layers.BatchNormalization(), 
	layers.Dense(256, activation='relu'), 
	layers.Dropout(0.3), 
	layers.BatchNormalization(), 
	layers.Dense(100, activation='softmax') 
]) 

model.compile( 
	loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), 
	optimizer='adam', 
	metrics=['AUC', 'accuracy'] 
) 



model.summary()


hist = model.fit(x_train, y_train, 
				epochs=5, 
				batch_size=64, 
				verbose=1, 
				validation_data=(x_val, y_val)) 
